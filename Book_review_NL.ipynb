{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Book review NL",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spdrnl/bert_multilingual/blob/master/Book_review_NL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj7QV6PWAz5b",
        "colab_type": "text"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbT8EGWRWoNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7TogsID0Kk7",
        "colab_type": "text"
      },
      "source": [
        "# Check the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHrLCqXr_n3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "6589de22-fc07-44ae-cb8d-9ac294326122"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cd5bb073bb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhYJ2VAqVYMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwJ8cEE90QP_",
        "colab_type": "text"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrxqncwoSipi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "77d3afc2-1df8-40ac-a549-17dcab7a5dcf"
      },
      "source": [
        "! wget https://github.com/benjaminvdb/110kDBRD/releases/download/v2.0/110kDBRD_v2.tgz\n",
        "! tar -zxf 110kDBRD_v2.tgz 110kDBRD/train\n",
        "! tar -zxf 110kDBRD_v2.tgz 110kDBRD/test\n",
        "! ls 110kDBRD"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-04 18:43:52--  https://github.com/benjaminvdb/110kDBRD/releases/download/v2.0/110kDBRD_v2.tgz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/168819565/a09c2700-96a1-11e9-9310-a218631917bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200904%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200904T184352Z&X-Amz-Expires=300&X-Amz-Signature=527d44f8faf3a5ddbd9e4d884ef6cfbbce28f0ec31f153074e7e4028ee5d88ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168819565&response-content-disposition=attachment%3B%20filename%3D110kDBRD_v2.tgz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-09-04 18:43:52--  https://github-production-release-asset-2e65be.s3.amazonaws.com/168819565/a09c2700-96a1-11e9-9310-a218631917bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200904%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200904T184352Z&X-Amz-Expires=300&X-Amz-Signature=527d44f8faf3a5ddbd9e4d884ef6cfbbce28f0ec31f153074e7e4028ee5d88ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=168819565&response-content-disposition=attachment%3B%20filename%3D110kDBRD_v2.tgz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.130.219\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.130.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79052852 (75M) [application/octet-stream]\n",
            "Saving to: ‘110kDBRD_v2.tgz.2’\n",
            "\n",
            "110kDBRD_v2.tgz.2   100%[===================>]  75.39M  60.1MB/s    in 1.3s    \n",
            "\n",
            "2020-09-04 18:43:54 (60.1 MB/s) - ‘110kDBRD_v2.tgz.2’ saved [79052852/79052852]\n",
            "\n",
            "test  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd-3uiY4AcT9",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZM5WFrEBH1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "model_name = 'bert-base-multilingual-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlqv5myOTKmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "795e5587-7543-498c-e099-b1dfd0671411"
      },
      "source": [
        "vocabulary = tokenizer.get_vocab()\n",
        "\n",
        "print(list(vocabulary.keys())[1000:1010])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ി', 'ീ', 'െ', 'േ', 'ൈ', 'ൗ', '൧', '൨', 'ൺ', 'ൻ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipEUwB32-z8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91229bed-3d1f-4ecc-e976-c672dc9156b8"
      },
      "source": [
        "tokenizer.get_vocab()['[CLS]']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJEE1roXHY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4fa4bd7-b3f1-4e0b-f612-3144f6e07d0c"
      },
      "source": [
        "tokenizer.get_vocab()['idee']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPqOaaAhZfNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1073e6d1-d0d5-4b20-8b79-17f0f28c41b7"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def read_file(file_name):\n",
        "  with open(file_name) as f:\n",
        "    text = f.read()\n",
        "  return text\n",
        "\n",
        "def get_file_contents(base_dir, train_test, label):\n",
        "  dir_name = base_dir + '/' + train_test + '/' + label\n",
        "  file_names = os.listdir(dir_name)\n",
        "  contents = [read_file(dir_name + '/' + file_name) for file_name in file_names]\n",
        "  return contents\n",
        "\n",
        "base_dir = '110kDBRD'\n",
        "\n",
        "train_txt_pos = get_file_contents(base_dir, 'train', 'pos')\n",
        "train_txt_neg = get_file_contents(base_dir, 'train', 'neg')\n",
        "test_txt_pos = get_file_contents(base_dir, 'test', 'pos')\n",
        "test_txt_neg = get_file_contents(base_dir, 'test', 'neg')\n",
        "\n",
        "train_txt = train_txt_pos + train_txt_neg\n",
        "train_labels = np.hstack([np.ones(len(train_txt_pos)), np.zeros(len(train_txt_neg))])\n",
        "test_txt = test_txt_pos + test_txt_neg\n",
        "test_labels = np.hstack([np.ones(len(test_txt_pos)), np.zeros(len(test_txt_neg))])\n",
        "\n",
        "print(f\"The number of train samples is {len(train_labels)}, {len(train_txt_pos)}+/{len(train_txt_neg)}-\")\n",
        "print(f\"The number of test samples is {len(test_labels)}, {len(test_txt_pos)}+/{len(test_txt_neg)}-\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of train samples is 20028, 10014+/10014-\n",
            "The number of test samples is 2224, 1112+/1112-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPChKGc2bVor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "for txts in [train_txt, test_txt]:\n",
        "  for txt in txts:\n",
        "    tokenized = tokenizer.tokenize(txt)\n",
        "    max_len = max(max_len, len(tokenized))\n",
        "\n",
        "print(f\"The maximum length in tokens is {max_len}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZXWx_t0m8A",
        "colab_type": "text"
      },
      "source": [
        "# Encode the data to word pieces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mf_oaKbLUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b8013585-674c-4947-d5db-775a8f33054a"
      },
      "source": [
        "max_len = 512\n",
        "train_encoded = tokenizer.batch_encode_plus(train_txt,\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len, \n",
        "                        pad_to_max_length = True, \n",
        "                        return_attention_mask = True, \n",
        "                        truncation = True)\n",
        "\n",
        "test_encoded = tokenizer.batch_encode_plus(test_txt,\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len, \n",
        "                        pad_to_max_length = True, \n",
        "                        return_attention_mask = True, \n",
        "                        truncation = True)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l-h6MSlo5Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label\n",
        "  \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded['input_ids'],\n",
        "                                                    train_encoded['attention_mask'],\n",
        "                                                    train_encoded['token_type_ids'],\n",
        "                                                    train_labels)).map(map_example_to_dict)\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_encoded['input_ids'],\n",
        "                                                    test_encoded['attention_mask'],\n",
        "                                                    test_encoded['token_type_ids'],\n",
        "                                                    test_labels)).map(map_example_to_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGf3e_S_OYmk",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nS7hxpUOqm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
        "number_of_epochs = 5\n",
        "\n",
        "# model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# choosing Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81vtl50PSfxD",
        "colab_type": "text"
      },
      "source": [
        "# Train model with transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUf0ghBJi6P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "bert_history = model.fit(train_dataset.shuffle(len(train_labels)).batch(batch_size), \n",
        "                         epochs=number_of_epochs, \n",
        "                         validation_data=test_dataset.batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLLH2u9u46R5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}